# ATD Ingestion Service

A high-performance, multi-threaded service that monitors Kafka topics for file ingestion requests and processes them into ClickHouse using the AppSentinels CLI (as-cli). Designed for scalable, tenant-aware data ingestion with automatic table management.

## Overview

The ATD Ingestion Service acts as a bridge between Kafka and ClickHouse, enabling:
- **Real-time Processing**: Continuously monitors Kafka topics for new files to ingest
- **Multi-tenant Support**: Automatically routes data to tenant-specific tables
- **True Multiprocessing**: Thread pool + subprocess architecture for maximum concurrency
- **Automatic Table Creation**: Creates ClickHouse tables on-demand per tenant
- **Docker-Ready**: Fully containerized with docker-compose support
- **Production-Grade**: Includes health checks, logging, and graceful shutdown

## Architecture

The service uses a hybrid thread + subprocess architecture for optimal performance:

```
Main Process (Kafka Consumer)
    │
    ├── Thread Pool (Lightweight coordination)
    │   ├── Worker Thread 1 ──→ subprocess: as-cli process 1
    │   ├── Worker Thread 2 ──→ subprocess: as-cli process 2
    │   ├── Worker Thread 3 ──→ subprocess: as-cli process 3
    │   └── Worker Thread 4 ──→ subprocess: as-cli process 4
    │
    └── Each as-cli subprocess runs independently (true multiprocessing)
```

This design provides:
- **Efficient Queue Management**: Threads handle Kafka message coordination
- **Process Isolation**: Each file ingestion runs in a separate process
- **No GIL Limitations**: CPU-intensive work happens in subprocesses
- **Configurable Parallelism**: Control concurrent ingestions via `thread_pool_size`

## Features

- Monitors Kafka topic `atd-topic` for incoming messages
- Hybrid thread + subprocess architecture for true multiprocessing
- Configurable parallelism (default: 4 concurrent ingestions)
- Process isolation for each file ingestion
- Graceful shutdown handling across all processes
- Comprehensive logging at both service and subprocess levels
- Docker support with docker-compose
- Systemd service integration

## Installation

### Option 1: Docker (Recommended)

1. Build and run with docker-compose:
```bash
docker-compose up -d
```

2. Or build manually:
```bash
docker build -t atd-ingestion:latest .
docker run -d --name atd-ingestion \
  -v $(pwd)/config:/app/config:ro \
  -v $(pwd)/logs:/app/logs \
  -v $(pwd)/data:/data:ro \
  atd-ingestion:latest
```

### Option 2: Local Installation

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Install appsentinels-cli (as-cli):
```bash
pip install appsentinels-cli
# Or download binary from releases
```

3. Configure the service by editing `config/config.yaml`

4. Install systemd service:
```bash
sudo cp atd-ingestion.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable atd-ingestion
```

## Configuration

Edit `config/config.yaml` to configure:
- Kafka connection settings
- ClickHouse connection details  
- Thread pool size (controls number of concurrent as-cli processes)
- as-cli executable path and options
- Logging settings
- Table management (auto-creation and naming)

For Docker deployment, use `config/config.docker.yaml` or mount your custom config.

### Performance Tuning

- `thread_pool_size`: Number of concurrent as-cli processes (default: 4)
  - Each thread manages one subprocess
  - Set based on CPU cores and expected load
  - Higher values = more parallel ingestions
- `batch_size`: Records per batch for each ingestion (default: 10000)
- `timeout`: Maximum time for each subprocess (default: 300s)

### Tenant Table Management

The service supports multi-tenant deployments with separate tables per tenant:

1. **Table Naming**: Tables are named as `{table_prefix}_{tenant}` (e.g., `atd_tenant1`)
2. **Automatic Creation**: When `auto_create_tables: true`, passes `--create-table-if-not-exists` flag to `as-cli`
3. **as-cli handles table creation**: The `as-cli ingest` command creates tables if they don't exist

When a message contains a `tenant` field, the service:
- Constructs table name as `{table_prefix}_{tenant}`
- Passes `--table atd_tenant1 --tenant tenant1` to as-cli
- Optionally adds `--create-table-if-not-exists` flag based on config

Example as-cli command generated by the service:
```bash
as-cli ingest records --file /path/file.parquet \
  --table-name atd_tenant1 \
  --create-table \
  --batch-size 10000
```

The service uses the existing `as-cli ingest records` command which:
- Automatically handles ClickHouse connection from as-cli config
- Creates tables if they don't exist (with `--create-table` flag)
- Supports batch processing for efficient ingestion
- Can use schema files for custom table structures

## Docker Usage

### Using docker-compose (includes Kafka):
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f atd-ingestion

# Stop services
docker-compose down
```

### Connect to existing Kafka:
Update `docker-compose.yml` to remove Kafka services and update the network configuration.

### Environment Variables:
- `LOG_LEVEL`: Set logging level (default: INFO)
- Mount volumes:
  - `/app/config`: Configuration files (read-only)
  - `/app/logs`: Log files
  - `/data`: Data files for ingestion (read-only)

## Project Structure

```
atd-ingestion/
├── src/atd_ingestion/      # Main package
│   ├── __init__.py         # Package initialization
│   ├── config.py           # Configuration management
│   ├── service.py          # Main service orchestration
│   ├── kafka_consumer.py   # Kafka consumer implementation
│   ├── worker.py           # Message processing workers
│   └── logging_setup.py    # Logging configuration
├── tests/                  # Unit tests
│   ├── test_config.py      # Configuration tests
│   └── test_worker.py      # Worker tests
├── config/                 # Configuration files
│   ├── config.yaml         # Local configuration
│   └── config.docker.yaml  # Docker configuration
├── main.py                 # Main entry point
├── setup.py                # Package setup
├── Makefile               # Development tasks
└── docker-compose.yml     # Docker orchestration

## Usage

### Run manually:
```bash
# Install the package
pip install -e .

# Run the service
python main.py --config config/config.yaml

# Or use the installed command
atd-ingestion --config config/config.yaml
```

### Development:
```bash
# Install with dev dependencies
make dev-install

# Run tests
make test

# Run linting
make lint

# Format code
make format

# Run locally
make run
```

### Run as systemd service:
```bash
sudo systemctl start atd-ingestion
sudo systemctl status atd-ingestion
sudo systemctl stop atd-ingestion
```

### View logs:
```bash
# Service logs
sudo journalctl -u atd-ingestion -f

# Application logs
tail -f logs/atd-ingestion.log
```

## Message Format

The service expects Kafka messages in JSON format matching the automated-threat pattern:
```json
{
  "filename": "/path/to/data/file.parquet",
  "format": "parquet",
  "size": 125,
  "tenant": "tenant_name"
}
```

Required fields:
- `filename`: Full path to the file to be ingested

Optional fields:
- `tenant`: Tenant identifier for multi-tenant deployments
- `format`: File format (for logging purposes)
- `size`: File size in bytes

## Technical Details

### Process Architecture

The service achieves true multiprocessing through:
1. **Main Process**: Runs Kafka consumer and manages the thread pool
2. **Worker Threads**: Lightweight coordination, each managing a work item
3. **Subprocess Execution**: Each thread spawns an `as-cli` subprocess for actual ingestion
4. **Process Isolation**: Each file ingestion runs in its own process with dedicated resources

With `thread_pool_size: 4`, you get up to 4 concurrent `as-cli` processes running independently.

### Dependencies

The service requires:
- **Python 3.11+**: For the main service
- **appsentinels-cli >= 2.2.0**: Base CLI framework
- **as-cli-ingest plugin**: Provides the `ingest records` command
- **Additional packages**: pandas, clickhouse-connect, pyarrow for data processing

All dependencies are automatically installed in the Docker image.

## Development

### Testing with Mock Producer

Use the included test producer:
```bash
python test_producer.py --num-messages 10
```

Or create custom test messages:
```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

message = {
    "filename": "/path/to/test/file.parquet",
    "format": "parquet",
    "size": 125,
    "tenant": "test_tenant"
}
producer.send('atd-topic', message)
producer.flush()
```

### Building for Production

Use the optimized production Dockerfile:
```bash
docker build -f Dockerfile.prod -t atd-ingestion:prod .
```