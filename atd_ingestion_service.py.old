#!/usr/bin/env python3
import json
import logging
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import signal
import sys
from typing import Dict, Any, Optional
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import yaml


class ATDIngestionService:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.running = True
        self.consumer = None
        self.thread_pool = None
        self.logger = self._setup_logging()
        self.processing_queue = Queue()
        self.active_tasks = set()
        
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from YAML file"""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        logger = logging.getLogger('atd-ingestion')
        logger.setLevel(self.config.get('log_level', 'INFO'))
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # File handler
        file_handler = logging.FileHandler(
            self.config.get('log_file', '/home/vinaypatil/infosys/atd-ingestion/logs/atd-ingestion.log')
        )
        file_handler.setLevel(logging.DEBUG)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger

    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully"""
        self.logger.info(f"Received signal {signum}. Shutting down...")
        self.running = False
        if self.consumer:
            self.consumer.close()
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
        sys.exit(0)

    def _create_kafka_consumer(self) -> KafkaConsumer:
        """Create and configure Kafka consumer"""
        kafka_config = self.config['kafka']
        return KafkaConsumer(
            kafka_config['topic'],
            bootstrap_servers=kafka_config['bootstrap_servers'],
            group_id=kafka_config.get('group_id', 'atd-ingestion-group'),
            auto_offset_reset=kafka_config.get('auto_offset_reset', 'latest'),
            enable_auto_commit=kafka_config.get('enable_auto_commit', True),
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

    def _process_file(self, file_path: str, tenant: Optional[str] = None) -> bool:
        """Process a single file using as-cli"""
        try:
            # Build as-cli command
            as_cli_config = self.config['as_cli']
            cmd = [
                as_cli_config['executable'],
                'ingest',
                'records',  # as-cli uses subcommands
                '--file', file_path
            ]
            
            # Add tenant-specific table if provided
            if tenant:
                # Determine table name based on tenant
                table_name = f"{self.config.get('table_prefix', 'atd')}_{tenant}"
                cmd.extend(['--table-name', table_name])
            else:
                # Use default table name if no tenant
                cmd.extend(['--table-name', self.config.get('default_table', 'atd_logs')])
            
            # Add create table flag if configured
            if self.config.get('auto_create_tables', True):
                cmd.append('--create-table')
            
            # Add batch size if configured
            if 'batch_size' in self.config:
                cmd.extend(['--batch-size', str(self.config['batch_size'])])
            
            # Add schema file if configured
            if 'schema_file' in self.config:
                cmd.extend(['--schema-file', self.config['schema_file']])
            
            # Add any additional as-cli arguments from config
            if 'additional_args' in as_cli_config:
                cmd.extend(as_cli_config['additional_args'])
            
            self.logger.info(f"Executing: {' '.join(cmd)}")
            
            # Execute as-cli subprocess
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=as_cli_config.get('timeout', 300)  # 5 minutes default timeout
            )
            
            if result.returncode == 0:
                self.logger.info(f"Successfully ingested file: {file_path}")
                self.logger.debug(f"Output: {result.stdout}")
                return True
            else:
                self.logger.error(f"Failed to ingest file: {file_path}")
                self.logger.error(f"Error: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"Timeout while processing file: {file_path}")
            return False
        except Exception as e:
            self.logger.error(f"Error processing file: {str(e)}")
            return False

    def _process_message(self, message_data: Dict[str, Any]) -> bool:
        """Process a message containing file information"""
        try:
            # Support multiple field names for backward compatibility
            filename = message_data.get('filename') or message_data.get('file_path')
            tenant = message_data.get('tenant')
            
            if not filename:
                self.logger.error("Message missing filename field")
                return False
            
            # Process only single file per message
            if isinstance(filename, list):
                self.logger.warning(f"Message contains multiple files, processing only first: {filename[0]}")
                filename = filename[0]
            
            # Process the file
            return self._process_file(filename, tenant)
                
        except Exception as e:
            self.logger.error(f"Error processing message: {str(e)}")
            return False

    def _worker_thread(self, thread_id: int):
        """Worker thread that processes messages from the queue"""
        self.logger.info(f"Worker thread {thread_id} started")
        
        while self.running:
            try:
                # Get message from queue with timeout
                message = self.processing_queue.get(timeout=1)
                
                if message is None:  # Poison pill to stop thread
                    break
                
                self.logger.info(f"Thread {thread_id} processing message: {message.value}")
                success = self._process_message(message.value)
                
                if success:
                    self.logger.info(f"Thread {thread_id} successfully processed message")
                else:
                    self.logger.error(f"Thread {thread_id} failed to process message")
                    # Could implement retry logic here if needed
                
            except Exception as e:
                if self.running:  # Only log if we're still running
                    self.logger.error(f"Thread {thread_id} error: {str(e)}")
        
        self.logger.info(f"Worker thread {thread_id} stopped")

    def run(self):
        """Main service loop"""
        self.logger.info("Starting ATD Ingestion Service")
        
        try:
            # Create Kafka consumer
            self.consumer = self._create_kafka_consumer()
            self.logger.info(f"Connected to Kafka topic: {self.config['kafka']['topic']}")
            
            # Create thread pool
            num_threads = self.config.get('thread_pool_size', 4)
            self.thread_pool = ThreadPoolExecutor(max_workers=num_threads)
            
            # Start worker threads
            for i in range(num_threads):
                self.thread_pool.submit(self._worker_thread, i)
            
            # Main consumer loop
            while self.running:
                try:
                    # Poll for messages
                    messages = self.consumer.poll(timeout_ms=1000)
                    
                    for topic_partition, records in messages.items():
                        for record in records:
                            self.logger.info(f"Received message: offset={record.offset}")
                            self.processing_queue.put(record)
                    
                except KafkaError as e:
                    self.logger.error(f"Kafka error: {str(e)}")
                    time.sleep(5)  # Wait before retrying
                    
        except Exception as e:
            self.logger.error(f"Fatal error in main loop: {str(e)}")
            raise
        finally:
            self.shutdown()

    def shutdown(self):
        """Cleanup resources"""
        self.logger.info("Shutting down ATD Ingestion Service")
        self.running = False
        
        # Send poison pills to stop worker threads
        if self.thread_pool:
            for _ in range(self.config.get('thread_pool_size', 4)):
                self.processing_queue.put(None)
            self.thread_pool.shutdown(wait=True)
        
        if self.consumer:
            self.consumer.close()
        
        self.logger.info("ATD Ingestion Service stopped")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ATD Ingestion Service")
    parser.add_argument(
        '--config',
        type=str,
        default='/home/vinaypatil/infosys/atd-ingestion/config/config.yaml',
        help='Path to configuration file'
    )
    
    args = parser.parse_args()
    
    service = ATDIngestionService(args.config)
    service.run()